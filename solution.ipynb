{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47c4a80",
   "metadata": {},
   "source": [
    "# Нейросеть для автодополнения текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a7407",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div class='alert alert-info'> \n",
    "\n",
    "Финальное задание спринта 2. \n",
    "\n",
    "Задача – создать нейросеть, которая на основе начала фразы предсказывает её продолжение. \n",
    "\n",
    "Последовательность работы: \n",
    "\n",
    "1. Взять датасет, очистить его, подготовить для обучения модели.\n",
    "2. Реализовать и обучить модель на основе рекуррентных нейронных сетей.\n",
    "3. Замерить качество разработанной и обученной модели.\n",
    "4. Взять более «тяжёлую» предобученную модель из Transformers и замерить её качество.\n",
    "5. Проанализировать результаты и дать рекомендации разработчикам: стоит ли использовать лёгкую модель или лучше постараться поработать с ограничениями по памяти и использовать большую предобученную.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780ae28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Загрузка библиотек, установка констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c242a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "37a8ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditionally, \n",
    "SEED = 42\n",
    "# Train mode; if 'preliminar', just verify the code, \n",
    "# if 'final' – train the ultimate models versions. \n",
    "# When 'preliminar' mode is switched on, reduce the\n",
    "# data volume to 1_000 tweets, batch size to 64 \n",
    "# and print some test messages. \n",
    "TRAIN_MODE = 'final' \n",
    "# name of transformer model\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "# batch size\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    BATCH_SIZE = 64\n",
    "else: \n",
    "    BATCH_SIZE = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf341cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfde3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Данные нужно \n",
    "\n",
    "- привести к нижнему регистру;\n",
    "- удалить ссылки, упоминания, эмодзи (по необходимости);\n",
    "- заменить нестандартные символы;\n",
    "- токенизировать текст.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11eb28",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3d3580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directory\n",
    "os.chdir('C:/Users/User/Yandex.Disk/DS.projects/LSTM.Praktikum/data')\n",
    "# read raw text data and save to array\n",
    "with open('raw_data.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_data = np.array(file.read().lower().splitlines())\n",
    "\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    raw_data = raw_data[:1_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1539c8",
   "metadata": {},
   "source": [
    "### Очистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2e449",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "\n",
    "Вычищает то, что следует удалить, со всех позиций (начало, середина, конец фразы). Проверено. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2c3083e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function for clearing and splitting of data\n",
    "def split_and_clean(row): \n",
    "    # remove the mentions (@*)\n",
    "    row = re.sub(r'@.*?\\s', '', row)\n",
    "    row = re.sub(r'@.*?\\Z', '', row)\n",
    "\n",
    "    # remove the URLs (http or www)\n",
    "    row = re.sub(r'www.*?\\s', '', row)\n",
    "    row = re.sub(r'http.*?\\s', '', row)\n",
    "    row = re.sub(r'www.*?\\Z', '', row)\n",
    "    row = re.sub(r'http.*?\\Z', '', row)\n",
    "\n",
    "    # remove emojies ('*...anything')\n",
    "    row = re.sub(r'\\*([^ ]+)\\s', '', row)\n",
    "    row = re.sub(r'\\*([^ ]+)\\Z', '', row)\n",
    "\n",
    "    # remove special symbols (&*;)\n",
    "    row = re.sub(r'&([^ ]+)\\;', '', row)\n",
    "\n",
    "    # remove everything except of letters and numbers\n",
    "    row = re.sub(r'[^a-z0-9\\s]', '', row)\n",
    "\n",
    "    # substitute the multiple spaces to single ones\n",
    "    row = re.sub(r'[\\s+]', ' ', row)\n",
    "\n",
    "    # split the strings by spaces\n",
    "    row = row.split(' ')\n",
    "\n",
    "    # remove the empty elements from lists\n",
    "    row = list(filter(None, row))\n",
    "    \n",
    "    return(row)\n",
    "\n",
    "raw_data = list(map(split_and_clean, raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ead9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result of raw data import (deliberabely without seed)\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    for _ in np.random.randint(0, len(raw_data), 10): \n",
    "        print(raw_data[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c18652",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Поскольку по условию задания модель получает на вход 3/4 исходного текста, фразы, в которых осталось три слова и менее, следует удалить. Более того, из-за использования `ROUGE-2` как одной из метрик, удалять надо тексты, в которых четвёртая часть равна двум словам, т.е., общая длина не менее шести. \n",
    "\n",
    "Итого: вычищаем из корпуса текстов все последовательности короче шести слов.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d41aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Количество фраз до удаления слишком коротких: {len(raw_data)}.')\n",
    "# drop too short phrases\n",
    "raw_data = [phrase for phrase in raw_data if len(phrase) > 5]\n",
    "\n",
    "# check the results\n",
    "print(f'Количество фраз после удаления слишком коротких: {len(raw_data)}.')\n",
    "if len(min(raw_data, key=len)) < 6: \n",
    "    print('Очистка от коротких фраз прошла с ошибкой.')\n",
    "else: \n",
    "    print('Все короткие фразы удалены.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "53ae1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed (cleaned) dataset\n",
    "with open('processed_data.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in raw_data:\n",
    "        file.write(' '.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025594e",
   "metadata": {},
   "source": [
    "### Разбиение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fe4b9",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "По условию задания, обучающая выборка 80%, валидационная и тестовая по 10%.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "919ffe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В обучающей выборке содержится 1280901 фраз.\n",
      "В валидационной выборке содержится 160113 фраз.\n",
      "В тестовой выборке содержится 160113 фраз.\n"
     ]
    }
   ],
   "source": [
    "# create train, test and valid datasets\n",
    "train, interhim = train_test_split(raw_data, train_size=0.8, random_state=SEED)\n",
    "valid, test = train_test_split(interhim, train_size=0.5, random_state=SEED)\n",
    "\n",
    "del interhim\n",
    "\n",
    "# check splitting\n",
    "print(f'В обучающей выборке содержится {len(train)} фраз.')\n",
    "print(f'В валидационной выборке содержится {len(valid)} фраз.')\n",
    "print(f'В тестовой выборке содержится {len(test)} фраз.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2ee6472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets to disk\n",
    "with open('train.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in train:\n",
    "        file.write(' '.join(row) + '\\n')\n",
    "with open('valid.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in valid:\n",
    "        file.write(' '.join(row) + '\\n')\n",
    "with open('test.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in test:\n",
    "        file.write(' '.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48a4a5",
   "metadata": {},
   "source": [
    "## Подготовка к токенизации данных и их загрузке в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae436da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7805902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results of tokenization\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(tokenizer.encode(\n",
    "        raw_data[np.random.randint(0, len(raw_data), 1).item()], \n",
    "        is_split_into_words=True, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors='pt')\n",
    "        )\n",
    "    print('Длина фразы после энкодинга: ', len(tokenizer.encode(\n",
    "        raw_data[np.random.randint(0, len(raw_data), 1).item()], \n",
    "        is_split_into_words=True, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors='pt')[0])\n",
    "        )\n",
    "    print(tokenizer.tokenize(\n",
    "        raw_data[np.random.randint(0, len(raw_data), 1).item()], \n",
    "        is_split_into_words=True, \n",
    "        return_tensors='pt'\n",
    "        )\n",
    "        )\n",
    "    print('Длина токенизированной фразы: ', len(tokenizer.tokenize(\n",
    "        raw_data[np.random.randint(0, len(raw_data), 1).item()], \n",
    "        is_split_into_words=True, \n",
    "        return_tensors='pt')\n",
    "        )\n",
    "        )\n",
    "    print(raw_data[np.random.randint(0, len(raw_data), 1).item()])\n",
    "    print(\n",
    "        'Длина исходной фразы: ', \n",
    "        len(raw_data[np.random.randint(0, len(raw_data), 1).item()])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    return tokenizer.encode(\n",
    "        row, is_split_into_words=True, add_special_tokens=True, return_tensors='pt', \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ded86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс датасета\n",
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer=tokenize, target_mode='single'):\n",
    "        # the list for pairs, including the start of tokenized text and their end\n",
    "        self.samples = []\n",
    "\n",
    "        for line in texts: \n",
    "            # tokenize the text\n",
    "            token_ids = tokenizer(line) \n",
    "            # create a context (the known 75% of tokens)\n",
    "            context = token_ids[0][0:(3 * len(token_ids[0]) // 4)] \n",
    "            if target_mode == 'complete': \n",
    "                # create a target (the last 25% of tokens which must be reconstructed)\n",
    "                target = token_ids[0][(3 * len(token_ids[0]) // 4):] \n",
    "            elif target_mode == 'single': \n",
    "                # create a target (the single token following to first 75% of tokens)\n",
    "                target = token_ids[0][(3 * len(token_ids[0]) // 4) ]\n",
    "            # join the 'context' and 'target' as tulpe and add to 'samples'\n",
    "            self.samples.append((context, target))\n",
    "           \n",
    "    def __len__(self):\n",
    "        # return the length of samples\n",
    "        return len(self.samples) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the context and target with given number ('idx')\n",
    "        x, y = self.samples[idx] \n",
    "        return {\n",
    "            'context': x.detach().clone(), \n",
    "            'target': y.detach().clone()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == 'preliminar': \n",
    "    print('Пример содержимого класса MaskedDataset: ')\n",
    "    print(MaskedDataset(train, tokenize)[np.random.randint(0, len(train), 1).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenized datasets\n",
    "train_tok = MaskedDataset(train, tokenize)\n",
    "valid_tok = MaskedDataset(valid, tokenize)\n",
    "test_tok = MaskedDataset(test, tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch): \n",
    "    # список текстов и классов из батча\n",
    "    contexts = [item['context'] for item in batch]\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "\n",
    "    # дополняем тексты в батче padding'ом\n",
    "    padded_contexts = pad_sequence(contexts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # lengths = [len(text) for text in texts]\n",
    "    lengths = torch.tensor([len(text) for text in contexts])\n",
    "    # считаем маски\n",
    "    masks = (padded_contexts != 0).long()\n",
    "\n",
    "    # возвращаем преобразованный батч\n",
    "    return padded_contexts, masks, lengths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852239c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == 'preliminar': \n",
    "    print('Размеры выдачи функции collate_fn (без разделения на батчи): ')\n",
    "    padded_contexts, masks, lengths, targets = collate_fn(train_tok)\n",
    "    print(padded_contexts.shape)\n",
    "    print(masks.shape)\n",
    "    print(lengths.shape)\n",
    "    print(targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_tok, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_tok, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "test_dataloader = DataLoader(\n",
    "    test_tok, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(f'Количество батчей в train_dataloader: {len(train_dataloader)}')\n",
    "    print(f'Размер батча равен {BATCH_SIZE}')\n",
    "    print()\n",
    "    for x_batch, masks, lengths, y_batch in train_dataloader: \n",
    "        print('Содержимое контекстов в батче: ')\n",
    "        print(x_batch)\n",
    "        print(f'Размерность тензора с контекстом: {x_batch.shape}')\n",
    "        print()\n",
    "        print('Содержимое таргетов в батче: ')\n",
    "        print(y_batch)\n",
    "        print(f'Размерность тензора с таргетом: {y_batch.shape}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52616c",
   "metadata": {},
   "source": [
    "## Рекуррентная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c42c4",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Условия выполнения этапа: \n",
    "\n",
    "- Напишите код модели на основе LSTM. \n",
    "- В методе `forward` модель должна принимать на вход последовательность токенов и предсказывать следующий токен.\n",
    "- Дополнительно для модели реализуйте метод генерации нескольких токенов.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686f8f2",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Последовательность работы модели: \n",
    "\n",
    "1. Модель получает на вход начальную последовательность токенов *X*.\n",
    "2. Затем она предсказывает вероятности следующего токена *P*(*w*<sub>n+1</sub>).\n",
    "3. Токен *w*<sub>n+1</sub>, имеющий наибольшую вероятность, добавляется к последовательности. \n",
    "4. Модель снова делает предсказание *P*(*w*<sub>n+2</sub>).\n",
    "5. Процесс повторяется, пока не выполнится одно из условий:\n",
    "    - сгенерирован токен окончания (например, `<eos>`)\n",
    "    - или достигнута максимальная длина генерации.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # out_dim for sum\n",
    "        # NB: change if summ will be changed to concatenation\n",
    "        out_dim = hidden_dim \n",
    "\n",
    "        # output linear layer\n",
    "        self.fc = nn.Linear(out_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, lengths): \n",
    "        # embed the text\n",
    "        emb = self.embedding(x) \n",
    "        if TRAIN_MODE == 'preliminar': \n",
    "            print('Размерность эмбеддинга: ', emb.shape)\n",
    "        pack = pack_padded_sequence(\n",
    "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        # get the output of recurrent layer ('out')\n",
    "        out, _ = self.rnn(pack) \n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        if TRAIN_MODE == 'preliminar': \n",
    "            print('Размерность выходных данных RNN после pad_packed_sequence: ', out.shape)\n",
    "\n",
    "        # скрытые состояния <MSAK> токена \n",
    "        # после двух проходов двунаправленной сети\n",
    "        hidden_forward = out[:, :, :out.size(2)//2]\n",
    "        hidden_backward = out[:, :, out.size(2)//2:]\n",
    "\n",
    "        # агрегация скрытых состояний в зависимости от self.combine\n",
    "        hidden_agg = hidden_forward + hidden_backward\n",
    "        if TRAIN_MODE == 'preliminar': \n",
    "            print('Размерность выхода скрытых слоёв: ', hidden_agg.shape)\n",
    "\n",
    "        linear_out = self.fc(hidden_agg)\n",
    "        if TRAIN_MODE == 'preliminar': \n",
    "            print('Размерность выхода линейного слоя: ', linear_out.shape)\n",
    "\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f39f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMClassifier(vocab_size=tokenizer.vocab_size)\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metric_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f8f50",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "\n",
    "Приведение размерности массива предсказанных токенов к размерности таргетов\n",
    "\n",
    "`preds = torch.argmax(x_output, dim=1)[:, 0]`\n",
    "\n",
    "вызывает опасения. Но другими способами у меня это сделать не получилось. Успокаивает то, что в массиве, соответствующему одной фразе, содержатся одинаковые токены, то есть, можно брать только первый из них, что я и сделал.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция замера лосса и accuracy for the single output token\n",
    "def evaluate_single_token(model, loader):\n",
    "    model.eval()\n",
    "    correct, total_batch_size = 0, 0\n",
    "    sum_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, masks, lengths, y_batch in loader:\n",
    "            # выход модели для входа x_batch\n",
    "            x_output = model.forward(x_batch, lengths)\n",
    "            # reshape\n",
    "            # logits = x_output.reshape(-1, x_output.size(-1))  # [batch*seq, vocab]\n",
    "            # logits = x_output.reshape(x_output.shape[0], -1)\n",
    "            logits = x_output.flatten(start_dim=1)\n",
    "            targets = y_batch.reshape(-1)\n",
    "            if TRAIN_MODE == 'preliminar':\n",
    "                print(f'Размерность таргета после reshape: {targets.shape}')\n",
    "            loss = criterion(logits, targets) # функция потерь\n",
    "            if TRAIN_MODE == 'preliminar':\n",
    "                print(f'Величина потерь: {loss}')\n",
    "            preds = torch.argmax(x_output, dim=1)[:, 0] # предсказанные токены\n",
    "            if TRAIN_MODE == 'preliminar': \n",
    "                print(f'Размерность прогноза: {preds.shape}')\n",
    "                print(f'Прогноз модели: {preds}')\n",
    "            correct += (preds == targets).sum().item() # количество верно угаданных токенов\n",
    "            total_batch_size += targets.size(0) # размер батча\n",
    "            sum_loss += loss.item() # суммарная функция потерь\n",
    "    \n",
    "    # лосс и accuracy\n",
    "    avg_loss = sum_loss / len(loader)\n",
    "    accuracy = correct / total_batch_size\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основной цикл обучения\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    n_epochs = 5\n",
    "else: \n",
    "    n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_lstm.train()\n",
    "    train_loss = 0.\n",
    "    for x_batch, masks, lengths, y_batch in tqdm(train_dataloader, leave=False):\n",
    "        optimizer.zero_grad() # обнуление градиентов оптимизатора\n",
    "        # выход модели для входа x_batch\n",
    "        x_output = model_lstm(x_batch, lengths)\n",
    "        if TRAIN_MODE == 'preliminar': \n",
    "            print('Размерность выхода модели: ', x_output.shape)\n",
    "        # logits = x_output.reshape(-1, x_output.size(-1))  # [batch*seq, vocab]\n",
    "        # logits = x_output.reshape(x_output.shape[0], -1)\n",
    "        logits = x_output.flatten(start_dim=1)\n",
    "        if TRAIN_MODE == 'preliminar': \n",
    "            print('Размерность логитов: ', logits.shape)\n",
    "        # targets   \n",
    "        targets = y_batch.reshape(-1)\n",
    "        if TRAIN_MODE == 'preliminar': \n",
    "            print('Размерность таргета после reshape: ', targets.shape)\n",
    "        # функция потерь\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward() # расчёт градиентов\n",
    "        optimizer.step() # обновление градиентов\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss, val_acc = evaluate_single_token(model_lstm, valid_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Val Accuracy: {val_acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
