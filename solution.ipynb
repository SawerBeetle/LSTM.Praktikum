{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47c4a80",
   "metadata": {},
   "source": [
    "# Нейросеть для автодополнения текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a7407",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div class='alert alert-info'> \n",
    "\n",
    "Финальное задание спринта 2. \n",
    "\n",
    "Задача – создать нейросеть, которая на основе начала фразы предсказывает её продолжение. \n",
    "\n",
    "Последовательность работы: \n",
    "\n",
    "1. Взять датасет, очистить его, подготовить для обучения модели.\n",
    "2. Реализовать и обучить модель на основе рекуррентных нейронных сетей.\n",
    "3. Замерить качество разработанной и обученной модели.\n",
    "4. Взять более «тяжёлую» предобученную модель из Transformers и замерить её качество.\n",
    "5. Проанализировать результаты и дать рекомендации разработчикам: стоит ли использовать лёгкую модель или лучше постараться поработать с ограничениями по памяти и использовать большую предобученную.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780ae28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Загрузка библиотек, установка констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c242a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "37a8ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditionally, \n",
    "SEED = 42\n",
    "# batch size\n",
    "BATCH_SIZE = 512\n",
    "# Train mode; if 'preliminar', just verify the code, \n",
    "# if 'final' – train the ultimate models versions. \n",
    "# When 'preliminar' mode is switched on, reduce the\n",
    "# data volume to 1000 tweets. \n",
    "TRAIN_MODE = 'preliminar' \n",
    "# name of transformer model\n",
    "MODEL_NAME = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf341cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfde3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Данные нужно \n",
    "\n",
    "- привести к нижнему регистру;\n",
    "- удалить ссылки, упоминания, эмодзи (по необходимости);\n",
    "- заменить нестандартные символы;\n",
    "- токенизировать текст.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3d3580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directory\n",
    "os.chdir('C:/Users/User/Yandex.Disk/DS.projects/LSTM.Praktikum/data')\n",
    "# read raw text data and save to array\n",
    "with open('raw_data.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_data = np.array(file.read().lower().splitlines())\n",
    "\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    raw_data = raw_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2c3083e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function for clearing and splitting of data\n",
    "def split_and_clean(row): \n",
    "    # remove the mentions (@*)\n",
    "    row = re.sub(r'@.*?\\s', '', row)\n",
    "    # remove the URLs (http or www)\n",
    "    row = re.sub(r'www.*?\\s', '', row)\n",
    "    row = re.sub(r'http.*?\\s', '', row)\n",
    "    # remove emojies ('*...anything')\n",
    "    row = re.sub(r'\\*([^ ]+)\\s', '', row)\n",
    "    # remove special symbols (&*;)\n",
    "    row = re.sub(r'&([^ ]+)\\;', '', row)\n",
    "    # remove everything except of letters and numbers\n",
    "    row = re.sub(r'[^a-z0-9\\s]', '', row)\n",
    "    # substitute the multiple spaces to single ones\n",
    "    row = re.sub(r'[\\s+]', ' ', row)\n",
    "    # split the strings by spaces\n",
    "    row = row.split(' ')\n",
    "    # remove the empty elements from lists\n",
    "    row = list(filter(None, row))\n",
    "    \n",
    "    return(row)\n",
    "\n",
    "raw_data = list(map(split_and_clean, raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ead9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'week', 'is', 'not', 'going', 'as', 'i', 'had', 'hoped']\n",
      "['yeah', 'i', 'know', 'it', 'was', 'horrible', 'ugh', 'saddening']\n",
      "['thanks', 'for', 'bursting', 'my', 'bubble']\n",
      "['i', 'cant', 'take', 'this', 'heat', 'its', 'like', 'an', 'oven', 'in', 'here', 'i', 'feel', 'sick', 'nwo']\n",
      "['might', 'be', 'getting', 'a', 'sore', 'throat', 'again']\n",
      "['sad', 'about', 'kutner', 'being', 'killed', 'off', 'my', 'fav', 'show', 'house']\n",
      "['got', 'the', 'ebay', 'blues', 'item', 'i', 'want', 'jumped', 'from', 'no', 'bidders', 'to', 'over', '100', 'in', 'an', 'hour', 'still', 'has', '3', 'hours', 'to', 'go', 'id', 'better', 'not', 'get', 'my', 'hopes', 'up']\n",
      "['well', 'there', 'was', 'this', 'really', 'cool', 'part', 'where', 'i', 'wont', 'spoil', 'it']\n",
      "['was', 'intending', 'to', 'finish', 'editing', 'my', '536page', 'novel', 'manuscript', 'tonight', 'but', 'that', 'will', 'probably', 'not', 'happen', 'and', 'only', '12', 'pages', 'are', 'left']\n",
      "['monkeys', 'i', 'just', 'found', 'out', 'you', 'my', 'twin', 'and', 'you', 'wont', 'even', 'write', 'back', 'im', 'heartbroken']\n"
     ]
    }
   ],
   "source": [
    "# check the result of raw data import (deliberabely without seed)\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    for _ in np.random.randint(0, len(raw_data), 10): \n",
    "        print(raw_data[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c18652",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Поскольку по условию задания модель получает на вход 3/4 исходного текста, фразы, в которых осталось три слова и менее, следует удалить. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "35d41aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фраз до удаления слишком коротких: 1000.\n",
      "Количество фраз после удаления слишком коротких: 935.\n",
      "Все короткие фразы удалены.\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество фраз до удаления слишком коротких: {len(raw_data)}.')\n",
    "# drop too short phrases\n",
    "raw_data = [phrase for phrase in raw_data if len(phrase) > 3]\n",
    "\n",
    "# check the results\n",
    "print(f'Количество фраз после удаления слишком коротких: {len(raw_data)}.')\n",
    "if len(min(raw_data, key=len)) < 4: \n",
    "    print('Очистка от коротких фраз прошла с ошибкой.')\n",
    "else: \n",
    "    print('Все короткие фразы удалены.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed (cleaned) dataset\n",
    "with open('processed_data.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in raw_data:\n",
    "        file.write(' '.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53486369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# add pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfedbdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 340,  340, 9853, 4686,   74, 1521, 1312,  750, 2035,  345, 1239, 1561,\n",
      "          284,  502, 7471]])\n",
      "15\n",
      "['Ġit', 'Ġit', 'Ġcounts', 'Ġid', 'k', 'Ġwhy', 'Ġi', 'Ġdid', 'Ġeither', 'Ġyou', 'Ġnever', 'Ġtalk', 'Ġto', 'Ġme', 'Ġanymore']\n",
      "15\n",
      "['it', 'it', 'counts', 'idk', 'why', 'i', 'did', 'either', 'you', 'never', 'talk', 'to', 'me', 'anymore']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# check the results of tokenization\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    print((tokenizer.encode(raw_data[11], is_split_into_words=True, add_special_tokens=True, return_tensors='pt')))\n",
    "    print(len(tokenizer.encode(raw_data[11], is_split_into_words=True, add_special_tokens=True, return_tensors='pt')[0]))\n",
    "    print((tokenizer.tokenize(raw_data[11], is_split_into_words=True, return_tensors='pt')))\n",
    "    print(len(tokenizer.tokenize(raw_data[11], is_split_into_words=True, return_tensors='pt')))\n",
    "    print((raw_data[11]))\n",
    "    print(len(raw_data[11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fe4b9",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "По условию задания, обучающая выборка 80%, валидационная и тестовая по 10%.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "919ffe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В обучающей выборке содержится 748 фраз.\n",
      "В валидационной выборке содержится 93 фраз.\n",
      "В тестовой выборке содержится 94 фраз.\n"
     ]
    }
   ],
   "source": [
    "# create train, test and valid datasets\n",
    "train, interhim = train_test_split(raw_data, train_size=0.8, random_state=SEED)\n",
    "valid, test = train_test_split(interhim, train_size=0.5, random_state=SEED)\n",
    "\n",
    "del interhim\n",
    "\n",
    "# check splitting\n",
    "print(f'В обучающей выборке содержится {len(train)} фраз.')\n",
    "print(f'В валидационной выборке содержится {len(valid)} фраз.')\n",
    "print(f'В тестовой выборке содержится {len(test)} фраз.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2ee6472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets to disk\n",
    "with open('train.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in train:\n",
    "        file.write(' '.join(row) + '\\n')\n",
    "with open('valid.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in valid:\n",
    "        file.write(' '.join(row) + '\\n')\n",
    "with open('test.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in test:\n",
    "        file.write(' '.join(row) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
