{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47c4a80",
   "metadata": {},
   "source": [
    "# Нейросеть для автодополнения текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a7407",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div class='alert alert-info'> \n",
    "\n",
    "Финальное задание спринта 2. \n",
    "\n",
    "Задача – создать нейросеть, которая на основе начала фразы предсказывает её продолжение. \n",
    "\n",
    "Последовательность работы: \n",
    "\n",
    "1. Взять датасет, очистить его, подготовить для обучения модели.\n",
    "2. Реализовать и обучить модель на основе рекуррентных нейронных сетей.\n",
    "3. Замерить качество разработанной и обученной модели.\n",
    "4. Взять более «тяжёлую» предобученную модель из Transformers и замерить её качество.\n",
    "5. Проанализировать результаты и дать рекомендации разработчикам: стоит ли использовать лёгкую модель или лучше постараться поработать с ограничениями по памяти и использовать большую предобученную.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780ae28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Загрузка библиотек, установка констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c242a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a8ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditionally, \n",
    "SEED = 42\n",
    "# batch size\n",
    "BATCH_SIZE = 128\n",
    "# Train mode; if 'preliminar', just verify the code, \n",
    "# if 'final' – train the ultimate models versions. \n",
    "# When 'preliminar' mode is switched on, reduce the\n",
    "# data volume to 5_000 tweets. \n",
    "TRAIN_MODE = 'preliminar' \n",
    "# name of transformer model\n",
    "MODEL_NAME = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf341cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfde3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Данные нужно \n",
    "\n",
    "- привести к нижнему регистру;\n",
    "- удалить ссылки, упоминания, эмодзи (по необходимости);\n",
    "- заменить нестандартные символы;\n",
    "- токенизировать текст.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11eb28",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directory\n",
    "os.chdir('C:/Users/User/Yandex.Disk/DS.projects/LSTM.Praktikum/data')\n",
    "# read raw text data and save to array\n",
    "with open('raw_data.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_data = np.array(file.read().lower().splitlines())\n",
    "\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    raw_data = raw_data[:5_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1539c8",
   "metadata": {},
   "source": [
    "### Очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3083e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function for clearing and splitting of data\n",
    "def split_and_clean(row): \n",
    "    # remove the mentions (@*)\n",
    "    row = re.sub(r'@.*?\\s', '', row)\n",
    "    row = re.sub(r'@.*?\\Z', '', row)\n",
    "\n",
    "    # remove the URLs (http or www)\n",
    "    row = re.sub(r'www.*?\\s', '', row)\n",
    "    row = re.sub(r'http.*?\\s', '', row)\n",
    "    row = re.sub(r'www.*?\\Z', '', row)\n",
    "    row = re.sub(r'http.*?\\Z', '', row)\n",
    "\n",
    "    # remove emojies ('*...anything')\n",
    "    row = re.sub(r'\\*([^ ]+)\\s', '', row)\n",
    "    row = re.sub(r'\\*([^ ]+)\\Z', '', row)\n",
    "\n",
    "    # remove special symbols (&*;)\n",
    "    row = re.sub(r'&([^ ]+)\\;', '', row)\n",
    "\n",
    "    # remove everything except of letters and numbers\n",
    "    row = re.sub(r'[^a-z0-9\\s]', '', row)\n",
    "\n",
    "    # substitute the multiple spaces to single ones\n",
    "    row = re.sub(r'[\\s+]', ' ', row)\n",
    "\n",
    "    # split the strings by spaces\n",
    "    row = row.split(' ')\n",
    "\n",
    "    # remove the empty elements from lists\n",
    "    row = list(filter(None, row))\n",
    "    \n",
    "    return(row)\n",
    "\n",
    "raw_data = list(map(split_and_clean, raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ead9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will', 'have', 'a', 'meeting', 'in', 'an', 'hour', 'to', 'explain', 'which', 'version', 'of', 'oaw', 'we', 'use', 'what', 'to', 'say', 'none', 'atm', 'its', 'just', 'a', 'heap', 'of', 'unbundled', 'emf', 'tools']\n",
      "['im', 'in', 'pain']\n",
      "['i', 'dont', 'think', 'there', 'is', 'any', 'kind', 'of', 'good', 'stroke', 'ill', 'wait', 'to', 'hear', 'from', 'you', 'i', 'love', 'that', 'little', 'cat', 'l', 'xxx']\n",
      "['just', 'realised', 'how', 'gutted', 'she', 'is', 'to', 'return', 'back', 'to', 'london', 'without', 'the', 'end']\n",
      "['seems', 'jruby', 'support', 'for', 'hpricot', 'is', 'now', 'two', 'versions', 'behind']\n",
      "['sad', 'that', 'the', 'time', 'shift', 'means', 'its', 'dark', 'when', 'we', 'go', 'home']\n",
      "['no', 'i', 'lost', 'a', 'loyal']\n",
      "['still', 'feeling', 'almost', 'entirely', 'overwhelmed', 'by', 'an', 'uncomfortable', 'desire', 'for', 'swift', 'and', 'violent', 'revenge']\n",
      "['i', 'just', 'cant', 'commit', 'the', 'time', 'though', 'my', 'play', 'time', 'isnt', 'the', 'same', 'as', 'everyone', 'elses']\n",
      "['really', 'now', 'time', 'for', 'sleep', 'dreaming', 'of', 'my', 'city', 'more', 'tattoos', 'and', 'other', 'great', 'things', 'waking', 'up', 'to', 'early', 'morning', 'sociology']\n"
     ]
    }
   ],
   "source": [
    "# check the result of raw data import (deliberabely without seed)\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    for _ in np.random.randint(0, len(raw_data), 10): \n",
    "        print(raw_data[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c18652",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Поскольку по условию задания модель получает на вход 3/4 исходного текста, фразы, в которых осталось три слова и менее, следует удалить. Более того, из-за использования `ROUGE-2` как одной из метрик, удалять надо тексты, в которых четвёртая часть равна двум словам, т.е., общая длина не менее шести. \n",
    "\n",
    "Итого: вычищаем из корпуса текстов все последовательности короче шести слов.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d41aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фраз до удаления слишком коротких: 5000.\n",
      "Количество фраз после удаления слишком коротких: 4239.\n",
      "Все короткие фразы удалены.\n"
     ]
    }
   ],
   "source": [
    "print(f'Количество фраз до удаления слишком коротких: {len(raw_data)}.')\n",
    "# drop too short phrases\n",
    "raw_data = [phrase for phrase in raw_data if len(phrase) > 5]\n",
    "\n",
    "# check the results\n",
    "print(f'Количество фраз после удаления слишком коротких: {len(raw_data)}.')\n",
    "if len(min(raw_data, key=len)) < 6: \n",
    "    print('Очистка от коротких фраз прошла с ошибкой.')\n",
    "else: \n",
    "    print('Все короткие фразы удалены.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ae1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed (cleaned) dataset\n",
    "with open('processed_data.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in raw_data:\n",
    "        file.write(' '.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025594e",
   "metadata": {},
   "source": [
    "### Разбиение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fe4b9",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "По условию задания, обучающая выборка 80%, валидационная и тестовая по 10%.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919ffe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В обучающей выборке содержится 3391 фраз.\n",
      "В валидационной выборке содержится 424 фраз.\n",
      "В тестовой выборке содержится 424 фраз.\n"
     ]
    }
   ],
   "source": [
    "# create train, test and valid datasets\n",
    "train, interhim = train_test_split(raw_data, train_size=0.8, random_state=SEED)\n",
    "valid, test = train_test_split(interhim, train_size=0.5, random_state=SEED)\n",
    "\n",
    "del interhim\n",
    "\n",
    "# check splitting\n",
    "print(f'В обучающей выборке содержится {len(train)} фраз.')\n",
    "print(f'В валидационной выборке содержится {len(valid)} фраз.')\n",
    "print(f'В тестовой выборке содержится {len(test)} фраз.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee6472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets to disk\n",
    "with open('train.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in train:\n",
    "        file.write(' '.join(row) + '\\n')\n",
    "with open('valid.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in valid:\n",
    "        file.write(' '.join(row) + '\\n')\n",
    "with open('test.txt', 'w+', encoding='utf-8') as file: \n",
    "    for row in test:\n",
    "        file.write(' '.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48a4a5",
   "metadata": {},
   "source": [
    "## Подготовка к токенизации данных и их загрузке в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae436da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# add pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7805902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  289,   692,   271,  1918,  3715,   481,  5938,   502, 15052,   284,\n",
      "          2342,   319,  2646,   266,   563,   318, 13445,  2005,   407,   503,\n",
      "           783]])\n",
      "21\n",
      "['Ġh', 'oll', 'is', 'Ġdeath', 'Ġscene', 'Ġwill', 'Ġhurt', 'Ġme', 'Ġseverely', 'Ġto', 'Ġwatch', 'Ġon', 'Ġfilm', 'Ġw', 'ry', 'Ġis', 'Ġdirectors', 'Ġcut', 'Ġnot', 'Ġout', 'Ġnow']\n",
      "21\n",
      "['hollis', 'death', 'scene', 'will', 'hurt', 'me', 'severely', 'to', 'watch', 'on', 'film', 'wry', 'is', 'directors', 'cut', 'not', 'out', 'now']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# check the results of tokenization\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(tokenizer.encode(\n",
    "        raw_data[11], \n",
    "        is_split_into_words=True, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors='pt')\n",
    "        )\n",
    "    print(len(tokenizer.encode(\n",
    "        raw_data[11], \n",
    "        is_split_into_words=True, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors='pt')[0])\n",
    "        )\n",
    "    print(tokenizer.tokenize(\n",
    "        raw_data[11], is_split_into_words=True, return_tensors='pt')\n",
    "        )\n",
    "    print(len(tokenizer.tokenize(\n",
    "        raw_data[11], is_split_into_words=True, return_tensors='pt'))\n",
    "        )\n",
    "    print(raw_data[11])\n",
    "    print(len(raw_data[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c31ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  703,  1312,   285,   824,   262, 39442,  5494,    86,   912,   340,\n",
      "           286,   743,  1312,  1265,    72, 18869,  3285]])\n"
     ]
    }
   ],
   "source": [
    "def tokenize(row):\n",
    "    return tokenizer.encode(\n",
    "        row, is_split_into_words=True, add_special_tokens=True, return_tensors='pt', \n",
    "        )\n",
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(tokenize(train[np.random.randint(0, len(train), 1).item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67ded86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс датасета\n",
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, target_mode='single'):\n",
    "        # the list for pairs, including the start of tokenized text and their end\n",
    "        self.samples = []\n",
    "\n",
    "        for line in texts: \n",
    "            # tokenize the text\n",
    "            token_ids = tokenize(line) \n",
    "            # create a context (the known 75% of tokens)\n",
    "            context = token_ids[0][0:(3 * len(token_ids[0]) // 4)] \n",
    "            if target_mode == 'complete': \n",
    "                # create a target (the last 25% of tokens which must be reconstructed)\n",
    "                target = token_ids[0][(3 * len(token_ids[0]) // 4):] \n",
    "            elif target_mode == 'single': \n",
    "                # create a target (the single token following to first 75% of tokens)\n",
    "                target = token_ids[0][(3 * len(token_ids[0]) // 4) ]\n",
    "            # join the 'context' and 'target' as tulpe and add to 'samples'\n",
    "            self.samples.append((context, target))\n",
    "           \n",
    "    def __len__(self):\n",
    "        # return the length of samples\n",
    "        return len(self.samples) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the context and target with given number ('idx')\n",
    "        x, y = self.samples[idx] \n",
    "        return {\n",
    "            'context': x.detach().clone(), \n",
    "            'target': y.detach().clone()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8734727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': tensor([  616, 12385,   626, 30662, 26197,   257,  1643,   618,  1312,   373,\n",
      "        10833,   616,  3290,   656,   262,  3996,   286,   616,  7779,   616,\n",
      "         8046,  1312]), 'target': tensor(760)}\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODE == 'preliminar': \n",
    "    print(MaskedDataset(train, tokenize)[np.random.randint(0, len(train), 1).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd59d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenized datasets\n",
    "train_tok = MaskedDataset(train, tokenize)\n",
    "valid_tok = MaskedDataset(valid, tokenize)\n",
    "test_tok = MaskedDataset(test, tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d6cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch): \n",
    "    # список текстов и классов из батча\n",
    "    contexts = [item['context'] for item in batch]\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "\n",
    "    # дополняем тексты в батче padding'ом\n",
    "    padded_contexts = pad_sequence(contexts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # lengths = [len(text) for text in texts]\n",
    "    lengths = torch.tensor([len(text) for text in contexts])\n",
    "    # считаем маски\n",
    "    masks = (padded_contexts != 0).long()\n",
    "\n",
    "    # возвращаем преобразованный батч\n",
    "    return padded_contexts, masks, lengths, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5712c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_tok, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_tok, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "test_dataloader = DataLoader(\n",
    "    test_tok, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52616c",
   "metadata": {},
   "source": [
    "## Рекуррентная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c42c4",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Условия выполнения этапа: \n",
    "\n",
    "- Напишите код модели на основе LSTM. \n",
    "- В методе `forward` модель должна принимать на вход последовательность токенов и предсказывать следующий токен.\n",
    "- Дополнительно для модели реализуйте метод генерации нескольких токенов.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686f8f2",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "\n",
    "Последовательность работы модели: \n",
    "\n",
    "1. Модель получает на вход начальную последовательность токенов *X*.\n",
    "2. Затем она предсказывает вероятности следующего токена *P*(*w*<sub>n+1</sub>).\n",
    "3. Токен *w*<sub>n+1</sub>, имеющий наибольшую вероятность, добавляется к последовательности. \n",
    "4. Модель снова делает предсказание *P*(*w*<sub>n+2</sub>).\n",
    "5. Процесс повторяется, пока не выполнится одно из условий:\n",
    "    - сгенерирован токен окончания (например, `<eos>`)\n",
    "    - или достигнута максимальная длина генерации.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ce491ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # out_dim for sum\n",
    "        out_dim = hidden_dim \n",
    "\n",
    "        # output linear layer\n",
    "        self.fc = nn.Linear(out_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, lengths): \n",
    "        # embed the text\n",
    "        emb = self.embedding(x) \n",
    "        pack = pack_padded_sequence(\n",
    "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        # get the output of recurrent layer ('out')\n",
    "        out, _ = self.rnn(pack) \n",
    "        out, _ = pad_packed_sequence(out)\n",
    "\n",
    "        # скрытые состояния <MSAK> токена \n",
    "        # после двух проходов двунаправленной сети\n",
    "        hidden_forward = out[:, :, :out.size(2)//2]\n",
    "        hidden_backward = out[:, :, out.size(2)//2:]\n",
    "\n",
    "        # агрегация скрытых состояний в зависимости от self.combine\n",
    "        hidden_agg = hidden_forward + hidden_backward\n",
    "\n",
    "        linear_out = self.fc(hidden_agg)\n",
    "\n",
    "        return linear_out\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86f39f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMClassifier(vocab_size=tokenizer.vocab_size)\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metric_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d94a1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция замера лосса и accuracy for the single output token\n",
    "def evaluate_single_token(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    sum_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, mask_batch, len_batch, y_batch in loader:\n",
    "            x_output = model.forward(x_batch) # выход модели для входа x_batch\n",
    "            loss = criterion(x_output, y_batch) # функция потерь\n",
    "            preds = torch.argmax(x_output, dim=1) # предсказанные токены\n",
    "            accuracy += (preds == y_batch).sum().item() # количество верно угаданных токенов\n",
    "            total_batch_size += y_batch.size(0) # размер батча\n",
    "            sum_loss += loss.item() # суммарная функция потерь\n",
    "    \n",
    "    # лосс и accuracy\n",
    "    avg_loss = sum_loss / len(loader)\n",
    "    accuracy = correct / total_batch_size\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ae3bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (26) to match target batch_size (128).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m optimizer.zero_grad() \u001b[38;5;66;03m# обнуление градиентов оптимизатора\u001b[39;00m\n\u001b[32m      9\u001b[39m x_output = model_lstm(x_batch, len_batch) \u001b[38;5;66;03m# выход модели для входа x_batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# функция потерь\u001b[39;00m\n\u001b[32m     11\u001b[39m loss.backward() \u001b[38;5;66;03m# расчёт градиентов\u001b[39;00m\n\u001b[32m     12\u001b[39m optimizer.step() \u001b[38;5;66;03m# обновление градиентов\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (26) to match target batch_size (128)."
     ]
    }
   ],
   "source": [
    "# Основной цикл обучения\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_lstm.train()\n",
    "    train_loss = 0.\n",
    "    for x_batch, mask_batch, len_batch, y_batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad() # обнуление градиентов оптимизатора\n",
    "        x_output = model_lstm(x_batch, len_batch) # выход модели для входа x_batch\n",
    "        loss = criterion(x_output, y_batch) # функция потерь\n",
    "        loss.backward() # расчёт градиентов\n",
    "        optimizer.step() # обновление градиентов\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss, val_acc = evaluate_single_token(model_lstm, valid_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Val Accuracy: {val_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab02cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 128, 50257])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(x_output.shape)\n",
    "print(y_batch.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
